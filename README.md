Domain-Specific LLM with LoRA & RAG
This project demonstrates how to fine-tune an open-source LLM using LoRA for parameter-efficient adaptation and combine it with a RAG pipeline for domain-specific Q&A.

Features:

LoRA fine-tuning for efficient domain adaptation

RAG pipeline with FAISS vector store

Interactive Streamlit app for Q&A

Lightweight and deployable on a single GPU

Tech Stack:

Python, Hugging Face Transformers & PEFT

LangChain / FAISS / ChromaDB

Streamlit for UI