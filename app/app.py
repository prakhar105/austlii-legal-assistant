"""
LoRA Chat App (Simple & Human-Friendly)
---------------------------------------
This script launches a small web interface where you can talk to your
LoRA-fine-tuned model.

- Base Model: OpenLLaMA 3B (quantized to fit 8GB GPU)
- LoRA Adapter: Your fine-tuned adapter
- Output: Model-generated responses without RAG/document retrieval
"""

import torch
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# -------------------------------------------------
# 1. Basic Configuration
# -------------------------------------------------
BASE_MODEL = "openlm-research/open_llama_3b"  # Your base model
LORA_PATH = "lora_model"                      # Folder where LoRA adapter is saved
MAX_NEW_TOKENS = 256                          # Max tokens the model can generate per prompt

print("ðŸš€ Launching LoRA Chat App...")

# -------------------------------------------------
# 2. Load Model in 4-bit (Memory Efficient for 8GB GPU)
# -------------------------------------------------
print("â³ Loading model in 4-bit quantization...")

# Quantization config: helps fit the model in small GPUs
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",               # Stable 4-bit quantization format
    bnb_4bit_compute_dtype=torch.bfloat16,   # Efficient computation type
    bnb_4bit_use_double_quant=True           # Saves even more memory
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load the base model in 4-bit quantization
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_config,
    device_map="auto"   # Automatically chooses GPU
)

# Load LoRA adapter on top of the base model
model = PeftModel.from_pretrained(model, LORA_PATH)
model.eval()
print("âœ… LoRA model ready! Running in 4-bit mode on your GPU.")

# -------------------------------------------------
# 3. Function to Generate Responses
# -------------------------------------------------
def chat_with_lora(user_input: str) -> str:
    """
    Takes user text input and returns a response generated by the LoRA model.
    """
    if not user_input.strip():
        return "Please type something to chat with the model."

    # Convert text to tokens and send to GPU
    inputs = tokenizer(user_input, return_tensors="pt").to("cuda")

    # Generate model response without gradient computation
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            do_sample=True,
            max_new_tokens=MAX_NEW_TOKENS,
            temperature=0.7,        # Slight randomness for natural output
            top_p=0.9,              # Nucleus sampling for diversity
            repetition_penalty=1.2  # Reduces repetitive loops
        )

    # Decode tokens to human-readable text
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# -------------------------------------------------
# 4. Launch Gradio Web App
# -------------------------------------------------
iface = gr.Interface(
    fn=chat_with_lora,
    inputs=gr.Textbox(
        label="Your Message",
        lines=2,
        placeholder="Ask me anything..."
    ),
    outputs=gr.Textbox(
        label="LoRA Model Response",
        lines=8
    ),
    title="LexiAUS â€“ AI Legal Assistant for Australian Law",
    description="LexiAUS is a LoRA-fine-tuned AI assistant trained on AustLII legal documents. It can summarize clauses, explain Australian law in plain English, and answer legal questions."
)

if __name__ == "__main__":
    iface.launch(share=True)
